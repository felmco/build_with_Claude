# 3.2 Entendiendo el Almacenamiento en Cach√© de Prompts

## Introducci√≥n
El almacenamiento en cach√© de prompts permite reutilizar grandes porciones de tu prompt a trav√©s de m√∫ltiples peticiones, reduciendo dr√°sticamente los costes (hasta un 90%) y la latencia (hasta un 80%) para contenido repetido.

## ¬øPor Qu√© Almacenamiento en Cach√© de Prompts?

### Sin Cach√©
Cada llamada a la API procesa el prompt completo desde cero:
```
Petici√≥n 1: Procesa 10,000 tokens ‚Üí Coste total
Petici√≥n 2: Procesa 10,000 tokens ‚Üí Coste total (¬°mismo contenido!)
Petici√≥n 3: Procesa 10,000 tokens ‚Üí Coste total (¬°mismo contenido!)
```

### Con Cach√©
Reutiliza porciones cacheadas:
```
Petici√≥n 1: Procesa 10,000 tokens ‚Üí Cach√©alos ‚Üí Coste total
Petici√≥n 2: Lee de cach√© ‚Üí ¬°90% reducci√≥n de coste!
Petici√≥n 3: Lee de cach√© ‚Üí ¬°90% reducci√≥n de coste!
```

## Comparaci√≥n de Costes

### Precios (Aproximados)
- **Tokens de entrada regulares**: $3 por mill√≥n de tokens
- **Escritura en cach√©**: $3.75 por mill√≥n de tokens (25% m√°s)
- **Lectura de cach√©**: $0.30 por mill√≥n de tokens (¬°90% menos!)

### Ejemplo de C√°lculo
Prompt de 10,000 tokens, usado 100 veces:

**Sin cach√©**:
```
100 peticiones √ó 10,000 tokens √ó $3/MTok = $3.00
```

**Con cach√©**:
```
Escritura: 1 √ó 10,000 √ó $3.75/MTok = $0.0375
Lecturas: 99 √ó 10,000 √ó $0.30/MTok = $0.297
Total: $0.0375 + $0.297 = $0.3345
Ahorro: $3.00 - $0.33 = $2.67 (¬°89% reducci√≥n!)
```

## C√≥mo Funciona el Cach√© de Prompts

### Puntos de Ruptura de Cach√© (Breakpoints)
Marca el contenido a ser cacheado usando `cache_control`:

```python
from anthropic import Anthropic

client = Anthropic()

message = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "You are an AI assistant with access to the following documentation...",
        },
        {
            "type": "text",
            "text": "<long documentation content here>",
            "cache_control": {"type": "ephemeral"}  # ¬°Cachea esto!
        }
    ],
    messages=[
        {"role": "user", "content": "Based on the docs, explain feature X"}
    ]
)
```

### Duraci√≥n del Cach√©
- **Duraci√≥n**: 5 minutos
- **Actualizaci√≥n**: Cada acierto de cach√© (hit) extiende la duraci√≥n por 5 minutos
- **M√°ximo**: Los cach√©s duran tanto como se usen dentro de ventanas de 5 minutos

## Ejemplo B√°sico de Cach√©

**simple_caching.py**:
```python
#!/usr/bin/env python3
"""Ejemplo b√°sico de cach√© de prompts"""

from anthropic import Anthropic
import time

client = Anthropic()

# Base de conocimiento grande para cachear
KNOWLEDGE_BASE = """
Python Programming Guide:
=========================

1. Variables and Data Types:
   - Strings: text data enclosed in quotes
   - Integers: whole numbers
   - Floats: decimal numbers
   - Lists: ordered collections [1, 2, 3]
   - Dictionaries: key-value pairs {"key": "value"}

2. Control Flow:
   - if/elif/else: conditional execution
   - for loops: iterate over sequences
   - while loops: repeat while condition is true

3. Functions:
   - def function_name(parameters):
   - return values
   - *args and **kwargs for flexible parameters

4. Classes:
   - class ClassName:
   - __init__ method for initialization
   - self parameter for instance reference

[... imagina que esto son 10,000+ tokens de documentaci√≥n ...]
"""

def ask_with_caching(question: str):
    """Preguntar con base de conocimiento cacheada"""

    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=1024,
        system=[
            {
                "type": "text",
                "text": "You are a Python programming expert. Use the following documentation to answer questions:"
            },
            {
                "type": "text",
                "text": KNOWLEDGE_BASE,
                "cache_control": {"type": "ephemeral"}  # Cachear este bloque
            }
        ],
        messages=[
            {"role": "user", "content": question}
        ]
    )

    # Comprobar uso de cach√©
    usage = response.usage
    print(f"""
 üìä Uso de Tokens:
    Tokens entrada: {usage.input_tokens}
    Creaci√≥n cach√©: {getattr(usage, 'cache_creation_input_tokens', 0)}
    Lectura cach√©: {getattr(usage, 'cache_read_input_tokens', 0)}
    Tokens salida: {usage.output_tokens}
    """)

    return response.content[0].text

def main():
    """Probar cach√© con m√∫ltiples peticiones"""

    questions = [
        "What are Python data types?",
        "Explain Python functions",
        "How do classes work in Python?",
        "What are control flow statements?"
    ]

    for i, question in enumerate(questions, 1):
        print(f"\n{'='*60}")
        print(f"Petici√≥n #{i}: {question}")
        print('='*60)

        answer = ask_with_caching(question)
        print(f"\nüí¨ Respuesta: {answer}")

        if i < len(questions):
            print("\n‚è≥ Esperando 1 segundo...")
            time.sleep(1)  # Peque√±o retraso entre peticiones

if __name__ == "__main__":
    main()
```

## Cacheando Prompts del Sistema

### Prompt del Sistema √önico
```python
message = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "Very long system instructions...",
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[{"role": "user", "content": "Question"}]
)
```

### M√∫ltiples Bloques del Sistema
```python
message = client.messages.create(
    model="claude-sonnet-4-5-20250929",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "General instructions (not cached)",
        },
        {
            "type": "text",
            "text": "Large knowledge base part 1...",
            "cache_control": {"type": "ephemeral"}  # Punto de cach√© 1
        },
        {
            "type": "text",
            "text": "Large knowledge base part 2...",
            "cache_control": {"type": "ephemeral"}  # Punto de cach√© 2
        }
    ],
    messages=[{"role": "user", "content": "Question"}]
)
```

## Cacheando Historial de Conversaci√≥n

### Cacheando Conversaciones Largas
```python
def chat_with_caching(messages: list, new_message: str):
    """Chat con historial de conversaci√≥n cacheado"""

    # A√±adir nuevo mensaje de usuario
    messages.append({
        "role": "user",
        "content": new_message
    })

    # Marcar los √∫ltimos turnos para cach√© (contexto de conversaci√≥n)
    # Clonar mensajes para evitar modificar original
    cached_messages = messages[:-1]  # Todo menos el √∫ltimo mensaje
    if cached_messages:
        # A√±adir control de cach√© al √∫ltimo mensaje antes del actual
        last_msg = cached_messages[-1].copy()
        if isinstance(last_msg["content"], str):
            last_msg["content"] = [
                {
                    "type": "text",
                    "text": last_msg["content"],
                    "cache_control": {"type": "ephemeral"}
                }
            ]
        cached_messages[-1] = last_msg

    # A√±adir mensaje actual sin control de cach√©
    cached_messages.append(messages[-1])

    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",
        max_tokens=1024,
        messages=cached_messages
    )

    # A√±adir respuesta del asistente al historial
    messages.append({
        "role": "assistant",
        "content": response.content[0].text
    })

    return response
```

## Mejores Pr√°cticas de Cach√©

### 1. Cachear Contenido Grande y Reutilizado
‚úÖ **Buenos candidatos para cach√©**:
- Documentaci√≥n grande
- Instrucciones del sistema
- Ejemplos few-shot
- Definiciones de herramientas
- Bases de conocimiento
- Historial de conversaci√≥n

‚ùå **Malos candidatos**:
- Prompts peque√±os (< 1000 tokens)
- Contenido √∫nico, de una sola vez
- Contenido que cambia frecuentemente

### 2. Posicionar Contenido Cacheado Estrat√©gicamente
```python
# ‚ùå Mal: Contenido variable al final del cach√©
system = [
    {
        "type": "text",
        "text": f"Large docs... User preferences: {user_prefs}",  # ¬°Cambia por usuario!
        "cache_control": {"type": "ephemeral"}
    }
]

# ‚úÖ Bien: Contenido estable en cach√©
system = [
    {
        "type": "text",
        "text": "Large docs...",  # Contenido estable
        "cache_control": {"type": "ephemeral"}
    },
    {
        "type": "text",
        "text": f"User preferences: {user_prefs}"  # Variable, no cacheado
    }
]
```

### 3. Cachear en Puntos de Ruptura (Breakpoints) Naturales
```python
system = [
    {
        "type": "text",
        "text": "Core instructions...",
    },
    {
        "type": "text",
        "text": "Documentation section 1...",
        "cache_control": {"type": "ephemeral"}  # Breakpoint 1
    },
    {
        "type": "text",
        "text": "Documentation section 2...",
        "cache_control": {"type": "ephemeral"}  # Breakpoint 2
    }
]
```

## Soluci√≥n de Problemas

### El Cach√© No Se Usa
**Problema**: `cache_read_input_tokens` es siempre 0

**Soluciones**:
1. Comprueba el tama√±o m√≠nimo del cach√© (debe ser sustancial)
2. Verifica que la duraci√≥n del cach√© no ha expirado (5 minutos)
3. Aseg√∫rate de que se env√≠a exactamente el mismo contenido
4. Confirma que `cache_control` est√° establecido correctamente

## Pr√≥ximos Pasos
- Aprende sobre [Estrategias de Optimizaci√≥n de Cach√©](./06_cache_optimization.md)
- Explora [T√©cnicas de Reducci√≥n de Costes](./07_cost_reduction.md)
- Prueba [Procesamiento por Lotes](./08_batch_processing.md)

## Recursos Adicionales
- [Documentaci√≥n Oficial de Cach√© de Prompts](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)
